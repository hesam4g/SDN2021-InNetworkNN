\section{Offloading Neural Networks into \textit{SmartNICs}}
\label{sec-motive}

\textcolor{red}{[Talk about the necessities for accelerating NN/DNNs. Introduce the different opportunities/options of accelerators we have (GPU,TPU). Make the case for using SmartNICs as accelerators. Describe the efforts people already did in that sense (this related work, e.g., gianni's paper goes here. Other (less) related work goes into related work section. Mention we want to shed light into the capabilities of SmartNICs as NN accelerators. Give a very high overview of our evaluation and evaluation goals (say we tested different neural network types on different SmartNICs to understand their performance). Evaluation details (i.e., methodology) will be described in the next section).]}

Emerging applications, e.g., Deep Learning based applications - those using DNN and video analytics and image classification, pose a challenge to cloud-based services. These applications demand high network bandwidth and low latency that may not be possible due to the long communication distance between mobile devices and cloud servers. One alternative is to deploy servers closer to mobile users, which is called edge computing. \smartnics also can be used as edge servers providing fast services to the mentioned applications. In a typical fashion without any \smartnic, the NIC, host's CPUs, and GPU/TPU (two standard accelerators for ANN) communicate through PCIe express slot. Once a packet reaches the NIC, it goes through PCIe to CPUs. Then it should be passed to GPU/TPU, again through PCIe. The response should traverse the reverse path, meaning PCIe is passed four times in this scenario. 
\par
What will happen if we execute the model on a \smartnic and avoiding the PCIe slot's overhead. Siracusano \etal \cite{siracusano2020running} conducted novel research focusing on running deep learning applications on \smartnics. The offloading inference phase of three network-based machine learning applications to \smartnics is studied in the mentioned work. They compared three implementations, on Netronome, on P4-NetFPGA, and HDL. In terms of IPC, CPUs of \smartnics are more efficient than hosts'. Therefore, \smartnics are viable candidates for processing those requests. Due to limited resources available in their hardware, they applied Binarized Neural Network to decrease the memory demand and resolving the lack of floating-point operations on \smartnics. However, they lost some accuracy because of quantization. Though, they can offer much higher throughput and less latency by applying their approach.
\par
In the following section, we evaluate different ANNs on a different \smartnic having enough resources and supporting floating-point operations to understand whether we can have quicker latency without losing latency. Moreover, we alleviate the host's CPUs by executing models into the \smartnic.